{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RE3NVLepRf-O2O2vLvTW8RtYjxKepHn0",
      "authorship_tag": "ABX9TyNXPWUP0fXC9Wxgb4xMA/uJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NamDo8467/101293009_COMP3123_Assignment1/blob/main/Web_Scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Useful links\n",
        "[Reddit API tutorial](https://www.youtube.com/watch?v=x9boO9x3TDA)\n",
        "\n",
        "[Reddit API Documentation](https://www.reddit.com/dev/api)\n",
        "\n",
        "[Reddit App](https://www.reddit.com/prefs/apps)\n",
        "\n",
        "[Reddit OAuth2 Github repo](https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example)"
      ],
      "metadata": {
        "id": "s1g_qbmwEXYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "UKHaYv0tFUKO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VspgEn-ael2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f2083d-cf60-458c-a23b-673b6fd68f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get access_token from Reddit API\n",
        "import requests\n",
        "client_auth = requests.auth.HTTPBasicAuth('lwl5lxEHUx_QqUF5gIgVKg', 'jHn1pV9hKk7vAesM_mlGZAaEj-2QEQ')\n",
        "post_data = {\"grant_type\": \"password\", \"username\": \"NamDough\", \"password\": \"Namdo2002\"}\n",
        "headers = {\"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n",
        "response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
        "\n",
        "access_token = response.json()['access_token']\n",
        "\n"
      ],
      "metadata": {
        "id": "6PtNbOOGgJty"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the subreddits from a user\n",
        "headers = {\"Authorization\": f\"bearer {access_token}\", \"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n",
        "response1 = requests.get(\"https://oauth.reddit.com/subreddits/mine/subscriber\", headers=headers)\n"
      ],
      "metadata": {
        "id": "F4of4c6iohJ5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "_l-iQEjqpfzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "spreadsheet_id = \"1Aghg_s9fVC7lDJCdjzs3-JTff_YXJkzM3rUC4B9h9og\"\n",
        "\n",
        "link_df = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=csv\")\n",
        "\n",
        "print(link_df)\n",
        "\n",
        "subreddit_column = link_df.loc[:, \"Subreddit\"]\n",
        "\n",
        "link_column = link_df.loc[:, \"Link\"]\n",
        "\n",
        "links = {\n",
        "    'user-info': \"https://oauth.reddit.com/api/v1/me\",\n",
        "    'user-karma':\"https://oauth.reddit.com/api/v1/me/karma\",\n",
        "    'user-friends':\"https://oauth.reddit.com/prefs/friends\",\n",
        "    'subreddit':{\n",
        "        'Tim Hortons':[],\n",
        "        'York University':[],\n",
        "        'Toronto':[],\n",
        "        'Web Dev':[],\n",
        "        'Python':[]\n",
        "    }\n",
        "}\n",
        "# for index, link in enumerate(link_column):\n",
        "#   if subreddit_column[index] not in links['subreddit']:\n",
        "#     links[\"subreddit\"][f'{subreddit_column[index]}'] = []\n",
        "#   else:\n",
        "#     links[\"subreddit\"][f'{subreddit_column[index]}'].append(link)\n"
      ],
      "metadata": {
        "id": "iuErORjnkVJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ef089c-4ebc-45f5-9010-79689be11914"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Subreddit                                               Link\n",
            "0          Toronto  https://www.reddit.com/r/toronto/comments/1eby...\n",
            "1          Toronto  https://www.reddit.com/r/toronto/comments/1eak...\n",
            "2          Toronto  https://www.reddit.com/r/toronto/comments/1ebr...\n",
            "3      Tim Hortons  https://www.reddit.com/r/TimHortons/comments/1...\n",
            "4  York University  https://www.reddit.com/r/yorku/comments/1edwen...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all comments from a specific post\n",
        "headers = {\"Authorization\": f\"bearer {access_token}\", \"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n",
        "response1 = requests.get(\"https://oauth.reddit.com/r/TimHortons/comments/1earicv/job_interview/\", headers=headers)\n",
        "\n",
        "res = response1.json() # res[0] is the information about the post, and res[1] is the information about the comments of the post\n",
        "\n",
        "post_info = res[0]\n",
        "comments = res[1].get(\"data\").get(\"children\")\n",
        "\n",
        "\n",
        "comment_arr = []\n",
        "\n",
        "subreddit_arr = []\n",
        "\n",
        "column_name = [\"Comment\", \"Subreddit\"]\n",
        "for comment in comments:\n",
        "  data = comment.get('data')\n",
        "  comment_and_subreddit = [data.get(\"body\"), data.get(\"subreddit\")]\n",
        "  comment_arr.append(comment_and_subreddit)\n",
        "\n",
        "df = pd.DataFrame(comment_arr, columns=column_name)\n",
        "\n",
        "print(df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cbDFu3cmzYMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff03210-03ee-491e-891d-6c5c617fe104"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Comment   Subreddit\n",
            "0   I‚Äôve asked silly questions like this to people...  TimHortons\n",
            "1   Not specific to Tim Hortons, but as someone wh...  TimHortons\n",
            "2   I was asked if I was staring at interviewers t...  TimHortons\n",
            "3   I‚Äôve asked these types of questions while inte...  TimHortons\n",
            "4   I was once asked if I could have any job in th...  TimHortons\n",
            "5   If you were a tree what kind of tree would you...  TimHortons\n",
            "6   The questions weren‚Äôt that deep bro. The super...  TimHortons\n",
            "7   When I applied for my job I had to make a vide...  TimHortons\n",
            "8   During an interview at McDonald‚Äôs when I was 1...  TimHortons\n",
            "9                      Yall complain about anything ü§£  TimHortons\n",
            "10  I find all these big brand companys always cha...  TimHortons\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using model (Distilbert) to classfy the text sentiment\n",
        "from transformers import pipeline\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "classification_machine = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "f0HY5G0kcehP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the comments from df\n",
        "texts = list(df[\"Comment\"])\n",
        "\n",
        "results = classification_machine(texts)\n",
        "\n",
        "\n",
        "for text, result in zip(texts, results):\n",
        "  print(f\"Text: {text}\")\n",
        "  print(\"Result: \", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yjtMDkXehFK",
        "outputId": "67e4ade1-a9b2-4e85-f202-ff4935a0c223"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I‚Äôve asked silly questions like this to people. I‚Äôve already made up my mind if I was hiring them before the interview.\n",
            "Result:  {'label': 'NEGATIVE', 'score': 0.9945656061172485}\n",
            "Text: Not specific to Tim Hortons, but as someone who has done a lot of interviews before, it can often be really hard to get a feel for someone‚Äôs actual personality during an interview, which is especially important for customer service jobs.  \n",
            "\n",
            "These type of questions are intended to throw you off, in a good way, break the tension, and also see what sort of personality and charisma you may have, to react to a question that you almost certainly wouldn‚Äôt have prepared for in advance.\n",
            "Result:  {'label': 'POSITIVE', 'score': 0.9482031464576721}\n",
            "Text: I was asked if I was staring at interviewers tits once lol.\n",
            "Result:  {'label': 'NEGATIVE', 'score': 0.9973764419555664}\n",
            "Text: I‚Äôve asked these types of questions while interviewing, it gives me an idea if they‚Äôre able to think on their feet in a sense. Plus it‚Äôs a fun out of the box way to get to know someone.\n",
            "Result:  {'label': 'POSITIVE', 'score': 0.9996869564056396}\n",
            "Text: I was once asked if I could have any job in the world, what it would it be. I said professional ice cream taster. I did not get the jo.\n",
            "Result:  {'label': 'NEGATIVE', 'score': 0.9993439316749573}\n",
            "Text: If you were a tree what kind of tree would you be?\n",
            "Result:  {'label': 'NEGATIVE', 'score': 0.9932785630226135}\n",
            "Text: The questions weren‚Äôt that deep bro. The super powers one is actually semi common\n",
            "Result:  {'label': 'NEGATIVE', 'score': 0.9753376245498657}\n",
            "Text: When I applied for my job I had to make a video for 1 minute discussing a topic that was given to me seconds before I had to record. It was there so they had an idea of how you spoke and if your English was good enough. My girlfriend also had to do one a few months ago for her new job. I can only guess it may have something to do with that. Because there can‚Äôt be any meaningful response to ‚Äúcat or dog‚Äù other that HOW the response is given\n",
            "Result:  {'label': 'NEGATIVE', 'score': 0.9992403984069824}\n",
            "Text: During an interview at McDonald‚Äôs when I was 15, I was asked what item I would want if I were trapped on a deserted island. I think it‚Äôs just to get an idea of your personality\n",
            "Result:  {'label': 'NEGATIVE', 'score': 0.996703565120697}\n",
            "Text: Yall complain about anything ü§£\n",
            "Result:  {'label': 'NEGATIVE', 'score': 0.9969291090965271}\n",
            "Text: I find all these big brand companys always chavs such ridiculous interview questions, I did an interview from Walmart that I remember thinking their questions were weird, \n",
            "\n",
            "I love a good local buisness interview where it‚Äôs just a legitimate conversation with the owner\n",
            "Result:  {'label': 'NEGATIVE', 'score': 0.9619266986846924}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User Roberta model to classify the text sentiment\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "roberta_machine = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "lrD8WCy-ii5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = roberta_machine([\"During an interview at McDonald‚Äôs when I was 15, I was asked what item I would want if I were trapped on a deserted island. I think it‚Äôs just to get an idea of your personality\"])\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8h5bW2Lj_dC",
        "outputId": "eb06ad6f-7142-478b-da0c-527d0dac0b01"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'neutral', 'score': 0.8059695959091187}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# headers = {\"Authorization\": f\"{access_token}\", \"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n",
        "response = requests.get(\"https://oauth.reddit.com/api/v1/me\", headers=headers)\n",
        "\n",
        "# response.text\n",
        "response.json()\n"
      ],
      "metadata": {
        "id": "Qf_KqHtsgl0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}